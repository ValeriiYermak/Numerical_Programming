{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "BPSxEUDx8bSp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from scipy.spatial.distance import cosine\n",
        "from google.colab import drive\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Data Frame\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "file_path='/content/drive/My Drive/Numeric_Programming/word_embeddings_subset.p'\n",
        "\n",
        "with open(file_path, 'rb') as f:\n",
        "  word_embeddings = pickle.load(f)\n",
        "\n",
        "# Extracting three-dimensional vectors\n",
        "words = list(word_embeddings.keys())\n",
        "print('The first 10 words from DataFrame')\n",
        "display(words[:10])\n",
        "\n",
        "embeddings = np.array([word_embeddings[word] for word in words]) # Take only first 3 measures\n",
        "\n",
        "df = pd.DataFrame(embeddings, index=words, columns=[f'dim_{i+1}' for i in range(300)])\n",
        "print(df.head(10))\n",
        "\n",
        "df.describe()\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UWGGmz8xHCmg",
        "outputId": "ccb399cb-7230-4562-fe9f-aeb6058d1450"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "The first 10 words from DataFrame\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['country',\n",
              " 'city',\n",
              " 'China',\n",
              " 'Iraq',\n",
              " 'oil',\n",
              " 'town',\n",
              " 'Canada',\n",
              " 'London',\n",
              " 'England',\n",
              " 'Australia']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
            "country   -0.080078  0.133789  0.143555  0.094727 -0.047363 -0.023560   \n",
            "city      -0.010071  0.057373  0.183594 -0.040039 -0.029785 -0.079102   \n",
            "China     -0.073242  0.135742  0.108887  0.083008 -0.127930 -0.227539   \n",
            "Iraq       0.191406  0.125000 -0.065430  0.060059 -0.285156 -0.102539   \n",
            "oil       -0.139648  0.062256 -0.279297  0.063965  0.044434 -0.154297   \n",
            "town       0.123535  0.159180  0.030029 -0.161133  0.015625  0.111816   \n",
            "Canada    -0.136719 -0.154297  0.269531  0.273438  0.086914 -0.076172   \n",
            "London    -0.267578  0.092773 -0.238281  0.115234 -0.006836  0.221680   \n",
            "England   -0.198242  0.115234  0.062500 -0.058350  0.226562  0.045898   \n",
            "Australia  0.048828 -0.194336 -0.041504  0.084473 -0.114258 -0.208008   \n",
            "\n",
            "              dim_7     dim_8     dim_9    dim_10  ...   dim_291   dim_292  \\\n",
            "country   -0.008545 -0.186523  0.045898 -0.081543  ... -0.145508  0.067383   \n",
            "city       0.071777  0.013306 -0.143555  0.011292  ...  0.024292 -0.168945   \n",
            "China      0.151367 -0.045654 -0.065430  0.034424  ...  0.140625  0.087402   \n",
            "Iraq       0.117188 -0.351562 -0.095215  0.200195  ... -0.100586 -0.077148   \n",
            "oil       -0.184570 -0.498047  0.047363  0.110840  ... -0.195312 -0.345703   \n",
            "town       0.039795 -0.196289 -0.039307  0.067871  ... -0.007935 -0.091797   \n",
            "Canada    -0.018677  0.006256  0.077637 -0.211914  ...  0.105469  0.030762   \n",
            "London    -0.251953 -0.055420  0.020020  0.149414  ... -0.008667 -0.008484   \n",
            "England   -0.062256 -0.202148  0.080566  0.021606  ...  0.135742  0.109375   \n",
            "Australia -0.164062 -0.269531  0.079102  0.275391  ...  0.021118  0.171875   \n",
            "\n",
            "            dim_293   dim_294   dim_295   dim_296   dim_297   dim_298  \\\n",
            "country   -0.244141 -0.077148  0.047607 -0.075195 -0.149414 -0.044189   \n",
            "city      -0.062988  0.117188 -0.020508  0.030273 -0.247070 -0.122559   \n",
            "China      0.152344  0.079590  0.006348 -0.037842 -0.183594  0.137695   \n",
            "Iraq      -0.123047  0.193359 -0.153320  0.089355 -0.173828 -0.054688   \n",
            "oil        0.217773 -0.091797  0.051025  0.061279  0.194336  0.204102   \n",
            "town      -0.265625  0.029297  0.089844 -0.049805 -0.202148 -0.079590   \n",
            "Canada    -0.039307  0.183594 -0.117676  0.191406  0.074219  0.020996   \n",
            "London    -0.053223  0.197266 -0.296875  0.064453  0.091797  0.058350   \n",
            "England   -0.121582  0.008545 -0.171875  0.086914  0.070312  0.003281   \n",
            "Australia  0.042236  0.221680 -0.239258 -0.106934  0.030884  0.006622   \n",
            "\n",
            "            dim_299   dim_300  \n",
            "country    0.097168  0.067383  \n",
            "city       0.076172 -0.234375  \n",
            "China      0.093750 -0.079590  \n",
            "Iraq       0.302734  0.105957  \n",
            "oil        0.235352 -0.051025  \n",
            "town       0.068848 -0.164062  \n",
            "Canada     0.285156 -0.257812  \n",
            "London     0.022583 -0.101074  \n",
            "England    0.069336  0.056152  \n",
            "Australia  0.051270 -0.135742  \n",
            "\n",
            "[10 rows x 300 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 243 entries, country to Funafuti\n",
            "Columns: 300 entries, dim_1 to dim_300\n",
            "dtypes: float32(300)\n",
            "memory usage: 286.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function for searching nearest word\n",
        "def find_closest_word(vector, word_embeddings):\n",
        "  min_distance = float('inf')\n",
        "  closest_word = None\n",
        "\n",
        "  for word, emb in word_embeddings.items():\n",
        "    emb_vector = emb # Extract only first three coordinates\n",
        "    if np.all(emb_vector==0):\n",
        "      continue\n",
        "    distance = cosine(vector, emb_vector)\n",
        "    if np.isnan(distance): # If distance = NaN - skip\n",
        "      continue\n",
        "    if distance < min_distance:\n",
        "      min_distance = distance\n",
        "      closest_word = word\n",
        "\n",
        "  return closest_word"
      ],
      "metadata": {
        "id": "bQCyNK86LCjx"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1 our function\n",
        "sample_vector = df.iloc[0].values # Take the first vector in Data Frame\n",
        "closest_word = find_closest_word(sample_vector, word_embeddings)\n",
        "print(f'Nearest word to {df.index[0]}: {closest_word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzIRcMevMccs",
        "outputId": "c18e770c-f8b9-4ec8-cf9f-18b6981e223e"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest word to country: country\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2 our function\n",
        "sample_vector = df.iloc[11].values # Take the first vector in Data Frame\n",
        "closest_word = find_closest_word(sample_vector, word_embeddings)\n",
        "print(f'Nearest word to {df.index[11]}: {closest_word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptMGM2yarK8",
        "outputId": "a8366309-b600-4794-aef5-c142141b3cd4"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest word to Pakistan: Pakistan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 3 our function\n",
        "sample_vector = df.iloc[16].values # Take the first vector in Data Frame\n",
        "closest_word = find_closest_word(sample_vector, word_embeddings)\n",
        "print(f'Nearest word to {df.index[16]}: {closest_word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voooozNLaqzW",
        "outputId": "402d6d0a-a4ea-41bc-b532-87492a438911"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest word to Afghanistan: Afghanistan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our function with different vectors\n",
        "\n",
        "test_words = random.sample(words, 15)\n",
        "\n",
        "for word in test_words:\n",
        "  if word in df.index:\n",
        "    sample_vector = df.loc[word].values\n",
        "    closest_word = find_closest_word(sample_vector, word_embeddings)\n",
        "    print(f'Nearest word to {word}: {closest_word}')\n",
        "  else:\n",
        "    print(f'Word \"{word}\" not found in embeddings.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXFRycOOcmqx",
        "outputId": "284c0795-b6b6-4cb5-e907-cbf7b0d85b2a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest word to Valletta: Valletta\n",
            "Nearest word to Nassau: Nassau\n",
            "Nearest word to Bahrain: Bahrain\n",
            "Nearest word to Belize: Belize\n",
            "Nearest word to Sudan: Sudan\n",
            "Nearest word to Belgium: Belgium\n",
            "Nearest word to Algeria: Algeria\n",
            "Nearest word to Hanoi: Hanoi\n",
            "Nearest word to Stockholm: Stockholm\n",
            "Nearest word to Russia: Russia\n",
            "Nearest word to Luanda: Luanda\n",
            "Nearest word to joyful: joyful\n",
            "Nearest word to Turkmenistan: Turkmenistan\n",
            "Nearest word to Honduras: Honduras\n",
            "Nearest word to Lima: Lima\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross product calculation function\n",
        "def find_orthogonal_word(word1, word2, word_embeddings):\n",
        "    if word1 not in word_embeddings or word2 not in word_embeddings:\n",
        "        return None\n",
        "\n",
        "    vector1 = np.array(word_embeddings[word1])  # Use entire vector (300D)\n",
        "    vector2 = np.array(word_embeddings[word2])  # Use entire vector (300D)\n",
        "\n",
        "    # Calculate the cosine similarity between each word and the average vector\n",
        "    average_vector = (vector1 + vector2) / 2\n",
        "\n",
        "    # Find the word with the maximum cosine distance from the average vector\n",
        "    max_distance = -1  # Start with very low distance\n",
        "    orthogonal_word = None\n",
        "\n",
        "    for word, emb in word_embeddings.items():\n",
        "        emb_vector = np.array(emb)\n",
        "        if np.all(emb_vector == 0):\n",
        "            continue\n",
        "        # Compute the cosine distance\n",
        "        distance = cosine(average_vector, emb_vector)\n",
        "        if distance > max_distance:\n",
        "            max_distance = distance\n",
        "            orthogonal_word = word\n",
        "\n",
        "    return orthogonal_word"
      ],
      "metadata": {
        "id": "ip_p2SRuUcoX"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test orthogonal word with specific pairs\n",
        "\n",
        "test_pairs = [('country', 'city'), ('happy', 'sad')]\n",
        "\n",
        "for word1, word2 in test_pairs:\n",
        "  if word1 in word_embeddings and word2 in word_embeddings:\n",
        "    orthogonal_word = find_orthogonal_word(word1, word2, word_embeddings)\n",
        "    print(f'Orthogonal word to \"{word1}\" and \"{word2}\": {orthogonal_word}')\n",
        "  else:\n",
        "    print(f'One of the words \"{word1}\" or \"{word2}\" not found.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmMp1OSyVl2C",
        "outputId": "4e63f8f1-cdff-4032-b820-4b9358d5e75d"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthogonal word to \"country\" and \"city\": Valletta\n",
            "Orthogonal word to \"happy\" and \"sad\": Ankara\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function for calculate angle between words\n",
        "def angle_between_words(word1, word2, word_embeddings):\n",
        "  if word1 not in word_embeddings or word2 not in word_embeddings:\n",
        "    return None\n",
        "  vector1 = np.array(word_embeddings[word1])\n",
        "  vector2 = np.array(word_embeddings[word2])\n",
        "  cos_theta = np.dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
        "  angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
        "  return np.degrees(angle)"
      ],
      "metadata": {
        "id": "jvvM9mYXWlKI"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function of angle between words\n",
        "for word1, word2 in test_pairs:\n",
        "  if word1 in word_embeddings and word2 in word_embeddings:\n",
        "    angle = angle_between_words(word1, word2, word_embeddings)\n",
        "    print(f'The angle between \"{word1}\" and \"{word2}\": {angle:.2f} degrees')\n",
        "  else:\n",
        "    print(f'One of the words \"{word1}\" or \"{word2}\" not found.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e9Z8BvmX7dZ",
        "outputId": "04c38da1-41e9-4f40-86e3-3057ccea5a47"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The angle between \"country\" and \"city\": 71.16 degrees\n",
            "The angle between \"happy\" and \"sad\": 57.62 degrees\n"
          ]
        }
      ]
    }
  ]
}